{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phil-mira/NLP_new/blob/main/SchoolGroupModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model for Each School Group (Primary, Middle, High) üè´\n",
        "\n",
        "This notebook outlines the training and evaluation of three seperate models that have each been trained on a subset of the data. This was done to explore whether an individual model might perform better than one that is able to explain topics at every grade level. To use this notebook you need to add the \"dataset.json\", \"original_data.json\" and \"test_question.json\" files from the repo to the notebook. Due to the size of the models only A100 GPUs avaliable on Colab can be used for inference/training."
      ],
      "metadata": {
        "id": "xWzIqCKO6Wzf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gRzvInFk818k"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install trl\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install accelerate\n",
        "!pip3 install autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ra24UYx7yxe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "import gc\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from awq import AutoAWQForCausalLM\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "import bitsandbytes\n",
        "from transformers import BitsAndBytesConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aWnKrxC71cj"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmsItCbTsidw"
      },
      "source": [
        "##Load the preference training dataset into the required format\n",
        "This has been adjusted from the AllGradeLevel notebook as it now creates a subset of the data for each school group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZMz41nL735R"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "def load_preference_dataset_expert(data_path, grade_path, required_expert):\n",
        "  \"\"\"\n",
        "  This function returns the dataset in the required format from the original\n",
        "  dataset. It produces a subset of the data for each school group.\n",
        "\n",
        "  Args:\n",
        "    file_path (str): The path to the original dataset.\n",
        "\n",
        "  Returns:\n",
        "    dataset_dict (dict): The dataset in the required format.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(data_path, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "  with open(grade_path, 'r', encoding='utf-8') as f:\n",
        "      grades = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "  dataset_dict = {\n",
        "      \"system\":[],\n",
        "      \"prompt\": [],\n",
        "      \"chosen\": [],\n",
        "      \"rejected\": []\n",
        "  }\n",
        "\n",
        "  for position, item in enumerate(data):\n",
        "\n",
        "    try:\n",
        "      current_grade = grades[position][\"grade_level\"]\n",
        "      if current_grade <= 5:\n",
        "        expert = \"Primary\"\n",
        "      elif current_grade <= 8:\n",
        "        expert = \"Middle\"\n",
        "      else:\n",
        "        expert = \"High\"\n",
        "\n",
        "\n",
        "    if expert == required_expert:\n",
        "      dataset_dict[\"system\"].append(\"\"\"You are a helpful AI assistant working\n",
        "                                        in a school setting that provides suitable\n",
        "                                        answers to a child's questions given their age.\"\"\")\n",
        "      dataset_dict[\"prompt\"].append(item[\"prompt\"][0][\"content\"])\n",
        "      dataset_dict[\"chosen\"].append(item[\"chosen\"][0][\"content\"])\n",
        "      dataset_dict[\"rejected\"].append(item[\"rejected\"][0][\"content\"])\n",
        "\n",
        "  return dataset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di9aaynnsw7O"
      },
      "source": [
        "## Format the instruction for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i270thQLMUwa"
      },
      "outputs": [],
      "source": [
        "def chatml_format(example):\n",
        "  \"\"\"\n",
        "  This function formats the instruction for the model.\n",
        "\n",
        "  Args:\n",
        "    example (dict): The example to be formatted.\n",
        "\n",
        "  Returns:\n",
        "    dict: The formatted example.\n",
        "  \"\"\"\n",
        "\n",
        "  message = {\"role\": \"system\", \"content\": example['system']}\n",
        "  system = tokenizer.apply_chat_template([message], tokenize=False)\n",
        "\n",
        "  # Format instruction\n",
        "  message = {\"role\": \"user\", \"content\": example['prompt']}\n",
        "\n",
        "  prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "  # Format chosen answer\n",
        "  chosen = example['chosen'] + \"<|im_end|>\\n\"\n",
        "\n",
        "  # Format rejected answer\n",
        "  rejected = example['rejected'] + \"<|im_end|>\\n\"\n",
        "\n",
        "  return {\n",
        "      \"prompt\": system + prompt,\n",
        "      \"chosen\": chosen,\n",
        "      \"rejected\": rejected,\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2da_Yj1tPnm"
      },
      "source": [
        "## Set up the data and models -- Training Pipeline  üöÄ\n",
        " Data is now split between primary, middle and high school levels to determine if a mix of experts style model may be more suitable. Several steps have been combined when compared with the AllGradeLevel notebook to allow multiple models to be trained and tested simultaneously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJp8xpMJ7KCk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
        "new_models = [[\"Primary\",\"EducationHermes-Primary-2.5-Mistral-7B\"],\n",
        "              [\"Middle\",\"EducationHermes-Middle-2.5-Mistral-7B\"],\n",
        "              [\"High\",\"EducationHermes-High-2.5-Mistral-7B\"]]\n",
        "\n",
        "\n",
        "for level, new_model in new_models:\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
        "\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.padding_side = \"left\"\n",
        "\n",
        "  dataset = load_preference_dataset_expert(\"dataset.json\", \"original_data.json\", level)\n",
        "\n",
        "  train_size = int(0.9 * len(dataset[\"prompt\"]))\n",
        "\n",
        "  train_dataset = Dataset.from_dict({\n",
        "      \"system\": dataset[\"system\"][:train_size],\n",
        "      \"prompt\": dataset[\"prompt\"][:train_size],\n",
        "      \"chosen\": dataset[\"chosen\"][:train_size],\n",
        "      \"rejected\": dataset[\"rejected\"][:train_size]\n",
        "  })\n",
        "\n",
        "  eval_dataset = Dataset.from_dict({\n",
        "      \"system\": dataset[\"system\"][train_size:],\n",
        "      \"prompt\": dataset[\"prompt\"][train_size:],\n",
        "      \"chosen\": dataset[\"chosen\"][train_size:],\n",
        "      \"rejected\": dataset[\"rejected\"][train_size:]\n",
        "  })\n",
        "\n",
        "\n",
        "  # Format dataset\n",
        "  train_dataset = train_dataset.map(\n",
        "      chatml_format,\n",
        "  )\n",
        "\n",
        "  eval_dataset = eval_dataset.map(\n",
        "      chatml_format,\n",
        "  )\n",
        "\n",
        "\n",
        "  # LoRA Configuration\n",
        "  peft_config = LoraConfig(\n",
        "      r=16,\n",
        "      lora_alpha=16,\n",
        "      lora_dropout=0.05,\n",
        "      bias=\"none\",\n",
        "      task_type=\"CAUSAL_LM\",\n",
        "      target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
        "    )\n",
        "\n",
        "  #Quantization config\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      llm_int8_threshold=6.0,\n",
        "      llm_int8_has_fp16_weight=False,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "  )\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config=bnb_config,\n",
        "      attn_implementation=\"flash_attention_2\",\n",
        "      torch_dtype=torch.bfloat16,\n",
        "      device_map=\"auto\",\n",
        "  )\n",
        "\n",
        "  model.config.use_cache = False\n",
        "\n",
        "  # Max numer of steps has been\n",
        "  training_args = DPOConfig(\n",
        "      per_device_train_batch_size=4,\n",
        "      gradient_accumulation_steps=4,\n",
        "      gradient_checkpointing=True,\n",
        "      learning_rate=5e-5,\n",
        "      lr_scheduler_type=\"cosine\",\n",
        "      max_steps=200,\n",
        "      save_strategy=\"no\",\n",
        "      logging_steps=1,\n",
        "      output_dir=new_model,\n",
        "      optim=\"paged_adamw_32bit\",\n",
        "      warmup_steps=100,\n",
        "      bf16=True,\n",
        "      report_to=\"wandb\",\n",
        "  )\n",
        "\n",
        "\n",
        "  # Create DPO trainer\n",
        "  dpo_trainer = DPOTrainer(\n",
        "      model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=eval_dataset,\n",
        "      processing_class=tokenizer,\n",
        "      peft_config=peft_config,\n",
        "  )\n",
        "\n",
        "  # Train the model\n",
        "  dpo_trainer.train()\n",
        "\n",
        "  dpo_trainer.model.save_pretrained(f\"Final_{level}\")\n",
        "  tokenizer.save_pretrained(f\"Final_{level}\")\n",
        "\n",
        "\n",
        "  del dpo_trainer, model\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # During training the models were uploded to huggingface, these lines of code\n",
        "  # have since been removed for testing purposes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Responses for the Trained Models for model evaluation\n",
        "The responses for the untrained model were computed in the AllGradesLevel notebook. The script extracts the grade from each of the questions in order to feed it into the correct model that has been trained on the data for the grade level posed. The results are saved to json file which will then be formatted to a text file for human evaluation."
      ],
      "metadata": {
        "id": "D1_A9CJC4VTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_models = [[\"Primary\",\"EducationHermes-Primary-2.5-Mistral-7B\"],\n",
        "                  [\"Middle\",\"EducationHermes-Middle-2.5-Mistral-7B\"],\n",
        "                  [\"High\",\"EducationHermes-High-2.5-Mistral-7B\"]]\n",
        "questions_file = \"test_questions.json\"\n",
        "results = {\"Question\": {}, \"experts_model\": {}}"
      ],
      "metadata": {
        "id": "owVxhPjwyww-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Extracts the grade from each of the questions given all questions follow the\n",
        "# same format\n",
        "def extract_grade(text):\n",
        "    grade_pattern = r\"(\\d+)(?:th|st|nd|rd)|(College)|(Kindergarten)\"\n",
        "    match = re.search(grade_pattern, text, re.IGNORECASE)\n",
        "    if match:\n",
        "        if match.group(3):\n",
        "            return 0\n",
        "        elif match.group(2):\n",
        "            return 13\n",
        "        try:\n",
        "            return int(match.group(1))\n",
        "        except ValueError:\n",
        "            return None\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "9uGA6HyEz_Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "\n",
        "with open(questions_file, 'r', encoding='utf-8') as f:\n",
        "        questions = json.load(f)\n",
        "\n",
        "for level, model in trained_models:\n",
        "  trained_model = AutoModelForCausalLM.from_pretrained(f\"philippe-miranthis/{model}\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(f\"philippe-miranthis/{model}\")\n",
        "\n",
        "  for idx, question in enumerate(questions):\n",
        "\n",
        "    current_grade = extract_grade(question)\n",
        "    if current_grade <= 5:\n",
        "      expert = \"Primary\"\n",
        "    elif current_grade <= 8:\n",
        "      expert = \"Middle\"\n",
        "    else:\n",
        "      expert = \"High\"\n",
        "\n",
        "    if expert == level:\n",
        "      message = [{\"role\": \"system\", \"content\": \"\"\"You are a helpful AI assistant\n",
        "                                                 working in a school setting that\n",
        "                                                 provides suitable answers to a\n",
        "                                                 child's questions given their age.\"\"\"},\n",
        "              {\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "      prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "      # Generate answer with trained model\n",
        "      pipeline_trained = transformers.pipeline(\n",
        "          \"text-generation\",\n",
        "          model=trained_model,\n",
        "          tokenizer=tokenizer\n",
        "      )\n",
        "\n",
        "      sequences_trained = pipeline_trained(\n",
        "          prompt,\n",
        "          do_sample=True,\n",
        "          temperature=0.7,\n",
        "          top_p=0.9,\n",
        "          num_return_sequences=1,\n",
        "          max_length=200,\n",
        "      )\n",
        "\n",
        "      results[\"experts_model\"][f\"question_{idx+1}\"] = sequences_trained[0]['generated_text']\n",
        "      results[\"Question\"][f\"question_{idx+1}\"] = question\n",
        "\n",
        "  del trained_model, tokenizer, pipeline_trained, sequences_trained\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# Save results to a JSON file\n",
        "with open(\"model_comparison_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(\"Comparison results saved to model_comparison_results.json\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YhjVR40Py7Wf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del pipeline_trained, sequences_trained\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "OnU0MNcuYnWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkPeQ3PBudEn"
      },
      "source": [
        "## Benchmark Individual Models\n",
        "This section benchmarks each of the individual models on standard benchmarks to assess if there is any loss in performance which might hinder the models ability to answer questions correctly.  EleutherAI LM_Evaluation harness has been utilized to streamline this process. [Link to their GitHub repo](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yv6fN8rZToro",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install lm-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5JI4jqYTn3h"
      },
      "outputs": [],
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=philippe-miranthis/EducationHermes-Primary-2.5-Mistral-7B \\\n",
        "  --tasks arc_challenge,hellaswag,gsm8k,mmlu_formal_logic,mmlu_high_school_world_history,mmlu_high_school_geography,mmlu_high_school_government_and_politics,mmlu_high_school_biology,mmlu_high_school_chemistry,mmlu_high_school_computer_science,mmlu_high_school_mathematics,mmlu_high_school_physics,sciq \\\n",
        "  --device cuda:0 \\\n",
        "  --batch_size 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dMKKgmJNT3E0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=philippe-miranthis/EducationHermes-Middle-2.5-Mistral-7B \\\n",
        "  --tasks arc_challenge,hellaswag,gsm8k,mmlu_formal_logic,mmlu_high_school_world_history,mmlu_high_school_geography,mmlu_high_school_government_and_politics,mmlu_high_school_biology,mmlu_high_school_chemistry,mmlu_high_school_computer_science,mmlu_high_school_mathematics,mmlu_high_school_physics,sciq \\\n",
        "  --device cuda:0 \\\n",
        "  --batch_size 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=philippe-miranthis/EducationHermes-High-2.5-Mistral-7B \\\n",
        "  --tasks arc_challenge,hellaswag,gsm8k,mmlu_formal_logic,mmlu_high_school_world_history,mmlu_high_school_geography,mmlu_high_school_government_and_politics,mmlu_high_school_biology,mmlu_high_school_chemistry,mmlu_high_school_computer_science,mmlu_high_school_mathematics,mmlu_high_school_physics,sciq \\\n",
        "  --device cuda:0 \\\n",
        "  --batch_size 4"
      ],
      "metadata": {
        "id": "gh30AC75_2ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOuxxJ/0AE5TWIra4UnJM7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}