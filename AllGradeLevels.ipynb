{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyME9zTIEG2JAU53cxrNifxH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phil-mira/NLP_new/blob/main/AllGradeLevels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model for all Age Groups at School üè´\n",
        "\n",
        "This notebook outlines the training and evaluation of the model that has been trained on all of the data. To use this notebook you need to add the \"dataset.json\", \"original_data.json\" and \"test_question.json\" files from the repo to the notebook. Due to the size of the models only A100 GPUs avaliable on Colab can be used for inference/training."
      ],
      "metadata": {
        "id": "gTmuAZIIE1cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install trl\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install accelerate\n",
        "!pip3 install autoawq"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gRzvInFk818k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "import gc\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from awq import AutoAWQForCausalLM\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "import bitsandbytes\n",
        "from transformers import BitsAndBytesConfig\n"
      ],
      "metadata": {
        "id": "8Ra24UYx7yxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "_aWnKrxC71cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the preference training dataset into the required format"
      ],
      "metadata": {
        "id": "kmsItCbTsidw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "def load_preference_dataset(file_path):\n",
        "  \"\"\"\n",
        "  This function returns the dataset in the required format from the original dataset.\n",
        "\n",
        "  Args:\n",
        "    file_path (str): The path to the original dataset.\n",
        "\n",
        "  Returns:\n",
        "    dataset_dict (dict): The dataset in the required format.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "  dataset_dict = {\n",
        "      \"system\":[],\n",
        "      \"prompt\": [],\n",
        "      \"chosen\": [],\n",
        "      \"rejected\": []\n",
        "  }\n",
        "\n",
        "  for item in data:\n",
        "      dataset_dict[\"system\"].append(\"\"\"You are a helpful AI assistant working\n",
        "                                        in a school setting that provides suitable\n",
        "                                        answers to a child's questions given their age.\"\"\")\n",
        "      dataset_dict[\"prompt\"].append(item[\"prompt\"][0][\"content\"])\n",
        "      dataset_dict[\"chosen\"].append(item[\"chosen\"][0][\"content\"])\n",
        "      dataset_dict[\"rejected\"].append(item[\"rejected\"][0][\"content\"])\n",
        "\n",
        "  return dataset_dict"
      ],
      "metadata": {
        "id": "FZMz41nL735R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format the instruction for the model"
      ],
      "metadata": {
        "id": "Di9aaynnsw7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatml_format(example):\n",
        "  \"\"\"\n",
        "  This function formats the instruction for the model.\n",
        "\n",
        "  Args:\n",
        "    example (dict): The example to be formatted.\n",
        "\n",
        "  Returns:\n",
        "    dict: The formatted example.\n",
        "  \"\"\"\n",
        "\n",
        "  message = {\"role\": \"system\", \"content\": example['system']}\n",
        "  system = tokenizer.apply_chat_template([message], tokenize=False)\n",
        "\n",
        "  # Format instruction\n",
        "  message = {\"role\": \"user\", \"content\": example['prompt']}\n",
        "\n",
        "  prompt = tokenizer.apply_chat_template([message], tokenize=False,\n",
        "                                         add_generation_prompt=True)\n",
        "\n",
        "  # Format chosen answer\n",
        "  chosen = example['chosen'] + \"<|im_end|>\\n\"\n",
        "\n",
        "  # Format rejected answer\n",
        "  rejected = example['rejected'] + \"<|im_end|>\\n\"\n",
        "\n",
        "  return {\n",
        "      \"prompt\": system + prompt,\n",
        "      \"chosen\": chosen,\n",
        "      \"rejected\": rejected,\n",
        "  }\n"
      ],
      "metadata": {
        "id": "i270thQLMUwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the data and models - Training Pipeline üöÄ\n"
      ],
      "metadata": {
        "id": "L2da_Yj1tPnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJp8xpMJ7KCk"
      },
      "outputs": [],
      "source": [
        "model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
        "\n",
        "new_model = \"EducationHermes-2.5-Mistral-7B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_preference_dataset(\"dataset.json\")\n",
        "\n",
        "train_size = int(0.9 * len(dataset[\"prompt\"]))\n",
        "\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"system\": dataset[\"system\"][:train_size],\n",
        "    \"prompt\": dataset[\"prompt\"][:train_size],\n",
        "    \"chosen\": dataset[\"chosen\"][:train_size],\n",
        "    \"rejected\": dataset[\"rejected\"][:train_size]\n",
        "})\n",
        "\n",
        "eval_dataset = Dataset.from_dict({\n",
        "    \"system\": dataset[\"system\"][train_size:],\n",
        "    \"prompt\": dataset[\"prompt\"][train_size:],\n",
        "    \"chosen\": dataset[\"chosen\"][train_size:],\n",
        "    \"rejected\": dataset[\"rejected\"][train_size:]\n",
        "})\n",
        "\n",
        "\n",
        "# Format dataset\n",
        "train_dataset = train_dataset.map(\n",
        "    chatml_format,\n",
        ")\n",
        "\n",
        "eval_dataset = eval_dataset.map(\n",
        "    chatml_format,\n",
        ")\n",
        "\n",
        "# LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
        "  )\n",
        "\n",
        "#Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_has_fp16_weight=False,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the trainers and train the models"
      ],
      "metadata": {
        "id": "vhSglF2ktXW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = DPOConfig(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=200,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=1,\n",
        "    output_dir=new_model,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    warmup_steps=100,\n",
        "    bf16=True,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "\n",
        "# Create DPO trainer\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "dpo_trainer.train()\n",
        "\n",
        "dpo_trainer.model.save_pretrained(\"Final\")\n",
        "okenizer.save_pretrained(\"Final\")\n",
        "\n",
        "import gc\n",
        "del dpo_trainer, model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# During training the models were uploded to huggingface, these lines of code\n",
        "# have since been removed for testing purposes\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fxghdZ98LcmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Responses for the Trained Model for model evaluation\n",
        "The results are saved to json file which will then be formatted to a text file for human evaluation. Responses for the base model are also generated for baseline comparison."
      ],
      "metadata": {
        "id": "zkPeQ3PBudEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = \"philippe-miranthis/EducationHermes-2.5-Mistral-7B\"\n",
        "base_model = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
        "questions_file = \"test_questions.json\""
      ],
      "metadata": {
        "id": "k-BPF2JTujVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_tokenizer = AutoTokenizer.from_pretrained(trained_model, trust_remote_code=False)\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(trained_model, trust_remote_code=False)\n"
      ],
      "metadata": {
        "id": "c1z9YObSr3zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(questions_file, 'r', encoding='utf-8') as f:\n",
        "        questions = json.load(f)\n",
        "\n",
        "results = {\"Question\": {}, \"trained_model\": {}, \"untrained_model\": {}}\n",
        "\n",
        "# Generate answers for each question\n",
        "for idx, question in enumerate(questions):\n",
        "\n",
        "  message = [{\"role\": \"system\", \"content\": \"\"\"You are a helpful AI assistant\n",
        "                                                 working in a school setting that\n",
        "                                                 provides suitable answers to a\n",
        "                                                 child's questions given their age.\"\"\"},\n",
        "            {\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "  prompt = trained_tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "  # Generate answer with trained model\n",
        "  pipeline_trained = transformers.pipeline(\n",
        "      \"text-generation\",\n",
        "      model=trained_model,\n",
        "      tokenizer=trained_tokenizer\n",
        "  )\n",
        "\n",
        "  sequences_trained = pipeline_trained(\n",
        "      prompt,\n",
        "      do_sample=True,\n",
        "      temperature=0.7,\n",
        "      top_p=0.9,\n",
        "      num_return_sequences=1,\n",
        "      max_length=200,\n",
        "  )\n",
        "\n",
        "  results[\"trained_model\"][f\"question_{idx+1}\"] = sequences_trained[0]['generated_text']\n",
        "\n",
        "# Save results to a JSON file\n",
        "with open(\"model_comparison_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(\"Comparison results saved to model_comparison_results.json\")"
      ],
      "metadata": {
        "id": "uNQCxyWci2wa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del trained_model, trained_tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "h3sWUlMtiwgV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "untrained_tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=False)\n",
        "untrained_model = AutoModelForCausalLM.from_pretrained(base_model, trust_remote_code=False)\n"
      ],
      "metadata": {
        "id": "qazqQBVJov_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(questions_file, 'r', encoding='utf-8') as f:\n",
        "        questions = json.load(f)\n",
        "\n",
        "results = {\"Question\": {}, \"trained_model\": {}, \"untrained_model\": {}}\n",
        "\n",
        "for idx, question in enumerate(questions):\n",
        "\n",
        "  message = [{\"role\": \"system\", \"content\": \"\"\"You are a helpful AI assistant\n",
        "                                                 working in a school setting that\n",
        "                                                 provides suitable answers to a\n",
        "                                                 child's questions given their age.\"\"\"},\n",
        "            {\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "  prompt = untrained_tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "\n",
        "  # Generate answer with untrained model or base model\n",
        "  pipeline_untrained = transformers.pipeline(\n",
        "      \"text-generation\",\n",
        "      model=untrained_model,\n",
        "      tokenizer=untrained_tokenizer\n",
        "  )\n",
        "\n",
        "  sequences_untrained = pipeline_untrained(\n",
        "      prompt,\n",
        "      do_sample=True,\n",
        "      temperature=0.7,\n",
        "      top_p=0.9,\n",
        "      num_return_sequences=1,\n",
        "      max_length=200,\n",
        "  )\n",
        "\n",
        "  results[\"untrained_model\"][f\"question_{idx+1}\"] = sequences_untrained[0]['generated_text']\n",
        "\n",
        "  results[\"Question\"][f\"question_{idx+1}\"] = question\n",
        "\n",
        "  # Save results to a JSON file\n",
        "with open(\"model_comparison_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(\"Comparison results saved to model_comparison_results.json\")"
      ],
      "metadata": {
        "id": "oOvOpQG0ip5x",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark the model comparing with base\n",
        "This section benchmarks the model on standard benchmarks to assess if there is any loss in performance which might hinder the models ability to answer questions correctly. EleutherAI LM_Evaluation harness has been utilized to streamline this process. [Link to their GitHub repo](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file)"
      ],
      "metadata": {
        "id": "uYp3Ge6xhpH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm-eval"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B3hdT_g2qIq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=philippe-miranthis/EducationHermes-2.5-Mistral-7B \\\n",
        "  --tasks arc_challenge,hellaswag,gsm8k,mmlu_formal_logic,mmlu_high_school_world_history,mmlu_high_school_geography,mmlu_high_school_government_and_politics,mmlu_high_school_biology,mmlu_high_school_chemistry,mmlu_high_school_computer_science,mmlu_high_school_mathematics,mmlu_high_school_physics,sciq \\\n",
        "  --device cuda:0 \\\n",
        "  --batch_size 4"
      ],
      "metadata": {
        "id": "2532KRsRp7A7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=teknium/OpenHermes-2.5-Mistral-7B \\\n",
        "  --tasks arc_challenge,hellaswag,gsm8k,mmlu_formal_logic,mmlu_high_school_world_history,mmlu_high_school_geography,mmlu_high_school_government_and_politics,mmlu_high_school_biology,mmlu_high_school_chemistry,mmlu_high_school_computer_science,mmlu_high_school_mathematics,mmlu_high_school_physics,sciq \\\n",
        "  --device cuda:0 \\\n",
        "  --batch_size 4"
      ],
      "metadata": {
        "id": "8LlzByCLqEo5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}